{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_to_French_NLP",
      "provenance": [],
      "collapsed_sections": [
        "RobiIGpcoXxZ",
        "ScrqKKHj0UNM",
        "vNNUMe-gpJww",
        "cOLBBYDU0Zpv",
        "imnPvT6XKuzv",
        "nIU8PmreATPs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "RobiIGpcoXxZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvK4y8-tn5OJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import ast\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Utility Functions and Classes**"
      ],
      "metadata": {
        "id": "ScrqKKHj0UNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_TextFileasDict(file_path):\n",
        "  \"\"\"\n",
        "  DESCR: This function loads an english and french words file into a dictionary\n",
        "\n",
        "  file_path: This is the path to the text file containing both english and french words\n",
        "  \n",
        "  Return: a dictionary with the english word as key and the french word as value\n",
        "          e.g 'yes' : 'oui'.\n",
        "  \"\"\"\n",
        "  WordDict = pd.read_csv(file_path, sep =  \"\\s\", header = None)\n",
        "  list_english = list(WordDict[0])\n",
        "  list_french = list(WordDict[1])\n",
        "\n",
        "  dict_lang = dict()\n",
        "  for i in range(len(list_english)):\n",
        "    dict_lang[list_english[i]] = list_french[i]\n",
        "\n",
        "  return dict_lang"
      ],
      "metadata": {
        "id": "l8DDhYOLu8B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to extract the training embeddings\n",
        "def TrainVector(WordDict, eng_embeddings, fren_embeddings):\n",
        "  \"\"\"\n",
        "  DESCR: This function gets the embedding vectors of some selected english and french words\n",
        "  \n",
        "  WordDict: a dictionary with the english word as key and the french word as value\n",
        "  eng_embeddings: comprehensive english word embeddings\n",
        "  fren_embeddings: comprehensive french word embeddings\n",
        "  \n",
        "  Return: X - Selected english word embeddings\n",
        "          y - Selected french word embeddings\n",
        "  \"\"\"\n",
        "  EngFran_tup = [(list(WordDict.keys())[i], list(WordDict.values())[i]) for i in range(len(WordDict))]\n",
        "  pair_lang = [(index, english, french) for (index, (english, french)) in enumerate(EngFran_tup) if french in fren_embeddings.keys() if english in eng_embeddings.keys()]\n",
        "\n",
        "  French_TrainEmbed = {}\n",
        "  English_TrainEmbed = {}\n",
        "\n",
        "  for (index, english, french) in pair_lang:\n",
        "    English_TrainEmbed[index] = eng_embeddings[english]\n",
        "    French_TrainEmbed[index] = fren_embeddings[french]\n",
        "\n",
        "  x = pd.DataFrame(English_TrainEmbed).T.values \n",
        "  y = pd.DataFrame(French_TrainEmbed).T.values\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "dKD5cbd9d82N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_matrices(en_fr,eng_embeddings, fren_embeddings):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        en_fr: English to French dictionary\n",
        "        fren_embeddings: French words to their corresponding word embeddings.\n",
        "        eng_embeddings: English words to their corresponding word embeddings.\n",
        "    Output: \n",
        "        X: a matrix where the columns are the English embeddings.\n",
        "        Y: a matrix where the columns correspong to the French embeddings.\n",
        "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # X_l and Y_l are lists of the english and french word embeddings\n",
        "    X_l = list()\n",
        "    Y_l = list()\n",
        "\n",
        "    # get the english words (the keys in the dictionary) and store in a set()\n",
        "    english_set = set(eng_embeddings.keys())\n",
        "\n",
        "    # get the french words (keys in the dictionary) and store in a set()\n",
        "    french_set = set(fren_embeddings.keys())\n",
        "\n",
        "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
        "    french_words = set(en_fr.values())\n",
        "\n",
        "    # loop through all english, french word pairs in the english french dictionary\n",
        "    for en_word, fr_word in en_fr.items():\n",
        "\n",
        "        # check that the french word has an embedding and that the english word has an embedding\n",
        "        if fr_word in french_set and en_word in english_set:\n",
        "\n",
        "            # get the english embedding\n",
        "            en_vec = eng_embeddings[en_word]\n",
        "\n",
        "            # get the french embedding\n",
        "            fr_vec = fren_embeddings[fr_word]\n",
        "\n",
        "            # add the english embedding to the list\n",
        "            X_l.append(en_vec)\n",
        "\n",
        "            # add the french embedding to the list\n",
        "            Y_l.append(fr_vec)\n",
        "\n",
        "    # stack the vectors of X_l into a matrix X\n",
        "    X = np.array(X_l)\n",
        "\n",
        "    # stack the vectors of Y_l into a matrix Y\n",
        "    Y = np.array(Y_l)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "VNCRAIyY49ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(Transform_Matrix, Xtrain_embed, Ytrain_embed):\n",
        "  \"\"\"\n",
        "  DESCR: This function computes the loss function\n",
        "\n",
        "  Transform_Matrix: The matrix that transforms the english vectors to the equivalent french vectors\n",
        "  Xtrain_embed - Selected english word embeddings\n",
        "  Ytrain_embed - Selected french word embeddings\n",
        "  \n",
        "  Return: The loss forbenius_norm(//XR - Y//)\n",
        "  \"\"\"\n",
        "  m = len(Xtrain_embed)\n",
        "\n",
        "  prod = np.dot(Xtrain_embed, Transform_Matrix)\n",
        "  diff = prod - Ytrain_embed\n",
        "\n",
        "  diff_square = np.square(diff)\n",
        "  loss = diff_square.sum()/m\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "oH23X6zJrNuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_func(Transform_Matrix, Xtrain_embed, Ytrain_embed):\n",
        "  \"\"\"\n",
        "  DESCR: This function computes the gradient of the loss\n",
        "\n",
        "  Transform_Matrix: The matrix that transforms the english vectors to the equivalent french vectors\n",
        "  Xtrain_embed - Selected english word embeddings\n",
        "  Ytrain_embed - Selected french word embeddings\n",
        "  \n",
        "  Return: The gradient of the loss (2/m)*X.T(XR - Y)\n",
        "  \"\"\"\n",
        "  prod = np.dot(Xtrain_embed, Transform_Matrix)\n",
        "  diff = prod - Ytrain_embed\n",
        "  m = len(Xtrain_embed)\n",
        "\n",
        "  grad = (2/m)*np.dot(Xtrain_embed.T, diff)\n",
        "  return grad\n"
      ],
      "metadata": {
        "id": "488SZzWLtUPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, learning_rate = 0.01, epochs = 100, random_state = 0, verbose = None):\n",
        "  \"\"\"\n",
        "  DESCR: This function computes the optimal  transformation matrix\n",
        "\n",
        "  X -  english train data word embeddings\n",
        "  Y -  french train data word embeddings\n",
        "  learning_rate: rate at which the transformation matrix will be updated\n",
        "  epochs: Number of iterations for updating the transformation matrix\n",
        "  random_state: controls the random selection of value for Transformation matrix if TransformMatrix_init = None\n",
        "  \n",
        "  Return: The transformation matrix\n",
        "  \"\"\"\n",
        "  TransMatrix = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "  for i in range(1, epochs + 1):\n",
        "    if (verbose and (i % verbose == 0)):\n",
        "      print(f\"Epoch {i} : Loss : {loss_func(TransMatrix, X, Y)} \")\n",
        "      \n",
        "    gradient = gradient_func(TransMatrix, X, Y)\n",
        "    TransMatrix -= learning_rate * gradient\n",
        "    \n",
        "  return TransMatrix"
      ],
      "metadata": {
        "id": "Nw51JRQMv4Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_true):\n",
        "  sum = 0\n",
        "  true_sum = 0\n",
        "  for i in range(len(y_pred)):\n",
        "    sum += 1;\n",
        "    if (y_pred[i] == y_true[i]):\n",
        "      true_sum += 1\n",
        "\n",
        "  return true_sum / sum"
      ],
      "metadata": {
        "id": "nvmf_8Pb5njY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector_1, vector_2):\n",
        "  \"\"\"\n",
        "  DESCR: This function computes the cosine of angle between two vectors\n",
        "\n",
        "  vector_1: The first vector\n",
        "  vector_2: The second vector\n",
        "  \n",
        "  Return: the cosine of the angle between vector_1 and vector_2\n",
        "  \"\"\"\n",
        "\n",
        "  cosine_sim = np.dot(vector_1, vector_2) / (np.linalg.norm(vector_1) * np.linalg.norm(vector_2))\n",
        "\n",
        "  return cosine_sim"
      ],
      "metadata": {
        "id": "7cNzvFLBCzmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Convert_to_French(english_word, transform_matrix, english_embeddings, french_embeddings, nearest_neigbors = 1):\n",
        "  \"\"\"\n",
        "  DESCR: This function changes an english word to a french word\n",
        "  \n",
        "  english_word: This is the english word to be changed\n",
        "  english_embeddings: comprehensive english word embeddings\n",
        "  french_embeddings: comprehensive french word embeddings\n",
        "  nearest_neigbors: Number of french words that are similar to the english word to be returned\n",
        "\n",
        "  \"\"\"\n",
        "  eng_word_embed = english_embeddings.get(english_word, np.zeros(300))\n",
        "  if (np.sum(eng_word_embed) == 0):\n",
        "    return list((\"WordNotFound\", 0))\n",
        "  else:\n",
        "    french_word_equiv = np.dot(eng_word_embed, transform_matrix)\n",
        "\n",
        "    word_similarity = {}\n",
        "    for french_word in french_embeddings.keys():\n",
        "      french_word_embed = french_embeddings[french_word]\n",
        "\n",
        "      similarity = cosine_similarity(french_word_embed, french_word_equiv)\n",
        "      word_similarity[french_word] = similarity\n",
        "\n",
        "    sort_orders = sorted(word_similarity.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    \n",
        "    return sort_orders[0:nearest_neigbors]"
      ],
      "metadata": {
        "id": "_yJLZnmf-DfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "vNNUMe-gpJww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "french_embeddings = pickle.load(open(\"data/fr_embeddings.p\", \"rb\"))\n",
        "english_embeddings = pickle.load(open(\"data/en_embeddings.p\", \"rb\"))\n",
        " "
      ],
      "metadata": {
        "id": "kA4yOp13pGrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WordDict_train = load_TextFileasDict(\"data/en-fr.train.txt\")\n",
        "WordDict_test = load_TextFileasDict(\"data/en-fr.test.txt\")"
      ],
      "metadata": {
        "id": "xkLLuE1KraHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The length of the train data english-french word dictionary: {len(WordDict_train)}\")\n",
        "print(f\"The length of the test data english-french word dictionary: {len(WordDict_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoyAKZ4YsDII",
        "outputId": "f71cc74b-6d79-4dbb-d9a4-39fb70e4c537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the train data english-french word dictionary: 5000\n",
            "The length of the test data english-french word dictionary: 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "cOLBBYDU0Zpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting Training DataFrame**"
      ],
      "metadata": {
        "id": "zf1ltZUXaxvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = get_matrices(WordDict_train, english_embeddings, french_embeddings)"
      ],
      "metadata": {
        "id": "riPfP7UWjPfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgW8zfvED0FY",
        "outputId": "635944e3-05d3-47fc-e861-3e1528bb3e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4932, 300), (4932, 300))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculating the transformation Matrix**"
      ],
      "metadata": {
        "id": "JziYb3a-prx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = gradient_descent(X = X_train,Y = Y_train, learning_rate = 0.5, epochs = 1000, random_state = 20, verbose = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po9ZH-Kj18Nj",
        "outputId": "ad4a4d58-c3ff-4825-edf4-4081b39a0790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100 : Loss : 16.28496970266297 \n",
            "Epoch 200 : Loss : 2.3811240429667477 \n",
            "Epoch 300 : Loss : 0.9235554276962724 \n",
            "Epoch 400 : Loss : 0.6565489131086197 \n",
            "Epoch 500 : Loss : 0.5922218106932381 \n",
            "Epoch 600 : Loss : 0.5738985726990419 \n",
            "Epoch 700 : Loss : 0.5680201758028866 \n",
            "Epoch 800 : Loss : 0.565963005683714 \n",
            "Epoch 900 : Loss : 0.5651970749430207 \n",
            "Epoch 1000 : Loss : 0.5648994072970945 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word conversion**"
      ],
      "metadata": {
        "id": "imnPvT6XKuzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_word = 'university'\n",
        "print(Convert_to_French(english_word = english_word, transform_matrix = m, english_embeddings = english_embeddings ,\n",
        "                        french_embeddings = french_embeddings, nearest_neigbors = 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEtQKmXEjx4g",
        "outputId": "6ee363a5-18b3-4f76-fc49-6ec5a21e844f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('université', 0.7852209345721833), ('universitaire', 0.7447127058746604), ('universités', 0.7340356792049815)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_word = 'professor'\n",
        "print(Convert_to_French(english_word = english_word, transform_matrix = m, english_embeddings = english_embeddings ,\n",
        "                        french_embeddings = french_embeddings, nearest_neigbors = 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Q3BYmeLfFi",
        "outputId": "7e1f53ed-ad84-4493-955d-40fdf5e7eb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('professeure', 0.7608659711755349), ('chercheur', 0.7166674788985664), ('université', 0.6532344159622178)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_word = 'cat'\n",
        "print(Convert_to_French(english_word = english_word, transform_matrix = m, english_embeddings = english_embeddings ,\n",
        "                        french_embeddings = french_embeddings, nearest_neigbors = 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_PmcU5tLkxY",
        "outputId": "f8171344-21f4-407c-daef-0f86cdcabfcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('chat', 0.687437472381717), ('chats', 0.6848670868670468), ('chien', 0.6842748490688814)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**"
      ],
      "metadata": {
        "id": "nIU8PmreATPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "french_words_test = WordDict_test.keys()\n",
        "english_words_pred = [Convert_to_French(word, m, english_embeddings , french_embeddings)[0][0] for word in french_words_test]"
      ],
      "metadata": {
        "id": "gHCrXrrXsQEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_words_test = list(WordDict_test.values())\n",
        "print(f\"Model Accuracy : {accuracy(english_words_pred, english_words_test) * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr7TZK8B1GMb",
        "outputId": "8e0deb03-eff3-41fc-e196-02d912d7d0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy : 41.8%\n"
          ]
        }
      ]
    }
  ]
}